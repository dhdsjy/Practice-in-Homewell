# （一）聚类算法：从0到1

## 写在前面

这已经是第三次想写这个系列了，每次总以代码能力差为托词，不肯走出舒适区，最后不了了之。所以到现在能力还没有多少提升，算法是一定要踏踏实实学习的，调包对能力几乎没有什么提升的，写算法代码对工程能力的提升也是有不少的提升作用。所以借着这个实习机会，再一次把算法基础好好拾起来！这一次一定不要放弃了，代码能力差，也要走出舒适区，一天不行用两天，两天不行用三天，一个人不行问别人，查资料！一定要非常熟悉数据挖掘中的常用算法，这样你才有算法迁移能力，才能提搞算法在实际业务中的运用能力。另外，这段时间统计学知识(Udacity)、算法(Udacity、NG)的学习可以结合科赛的金融数据的入门教程以及Kaggle上的入门教程，理论结合实际。至于HTML、CSS、JS等爬虫知识的学习（七月算法、网络上的一个爬虫教程）可以跟着导航学习。*警示：如果你连基本算法都不能进行代码复现，论文中的代码你怎么实现？所以周志华老师那本书一定要肯透（一年多的时间是肯定要下的）*。 无论如何都要写出自己的一个代码版本，无论代码多么low，一定要写出来！要放弃的时候在撸一遍原理，抠细节，千万不要直接看标准答案的代码！先自己撸，照着原理撸，撸不出来，网上查资料，求助别人。把传统常见的代码再撸一遍并不是要让你写出多么优化的代码，做这个工作主要是为了让你非常熟悉基本的机器学习算法过程，就像1+1=2那样平常，更进一层，可以参考sklearn的代码，提高自己的代码水平。**没有对算法的透彻理解，你根本不知道如何应用它，什么时候用它！**

## 我印像中的聚类算法

目前我所遇见的聚类算法有：K-means（K-means++）、层次聚类、基于密度的聚类、谱聚类，其中最熟悉的肯定是Kmeans，然而kmeans++一点也不了解。。其他的都只是大概了解，知道有这么个东西。。。

### Kmeans算法回顾

**算法思想** 通过各种距离（欧式、余弦）计算两个点之前的距离，将距离最近的点归为一类，概括为：*计算质心-分配-重新计算*

**缺点** 只能聚成球形簇

**计算步骤**

+ 随机选择k个聚类中心，分别计算其他点到k个聚类中心间的距离（这k可通过肘部法则确定，具体的没有自己编写过，所以原理不是很清楚）
+ 分别将离聚类中心点最近的一个点归类为相应的簇中并计算更新后的聚类中心
+ 重复上述步骤，~~直到所有的点都已归类~~ 直到所有数据点的簇分配结果不再改变为止

## 聚类算法总览

![img](https://pic1.zhimg.com/50/1dde4f5e8f0cf383b37c93f812d2e40c_hd.jpg)

#### 相似性度量

+ 距离 L1-norm 绝对值/曼哈顿距离 L2-norm 欧氏距离 Mahalanobis距离->应用于GMM
+ 相似系数 主要是夹角余弦和相关系数 ，相关系数的应用比较广泛，其主要优势是不受线性变换的影响
+ 核函数
+ DTW

#### 聚类算法

+ **Hierarchical methods** 主要有两种路径：bottom-up top-down。其中比较新的算法有：
  + BIRCH针对较大类型的数据且数据类型为numerical
  + ROCK 主要用在categorical
  + chameleon 用knn做linkage，构建一个graph，聚类效果非常好
+ **Partition-based methods**
  + kmeans 对初始值设置很敏感，所以有了kmeans++ intelligent kmeans genetic kmeans 
  + kmeans 对噪声和离群点很敏感，所以有了kmedoids kmedians
  + categorical类型数据的聚类 kmodes
  + 解决非凸型数据 kernel kmeans
+ **Density-based methods** 解决不规则的聚类问题，同时对噪声数据处理也较好，主要有两个参数，一个是圈子的最大半径以及圈子中最少容纳点的个数
  + DBSCAN 其扩展算法OPTICS优先对高密度进行搜索，然后根据高密度的特点设置参数，改善DBSCAN的不足
+ **Grid-based methods** 这类方法的原理是将数据空间划分为网格单元，将数据对象映射到网格单元中，并计算每个单元的密度，根据预设的阈值判断每个网格单元是否为高密度单元，由临近的稠密单元形成类。该类方法的优点是执行效率高，因为其速度与数据对象的个数无关，而只依赖于数据空间每个维上单元的个数，缺点是对参数敏感、无法处理不规则数据、维数灾难
  + STING
  + CLIQUE
+ **Model-based method** 这一类方法主要是指基于概率模型的方法和基于神经网络模型的方法，尤其以基于概率模型的方法居多。 这里的概率模型主要指概率生成模型，同一类的数据属于同一种概率分布。这种方法的优点是对类的划分不难么坚硬，而是以概率的形式出现，每一类的特征也可以用参数表达，缺点是执行效率不高，特别是分布数量很多并且数量很少的时候。
  + GMM
  + SOM 基于神经网络模型的方法

#### 数据简化

有些算法需要对数据做简化，才得以具备处理大数据的能力，比如BIRCH

+ **变换**
  + 离散傅里叶变换 提取数据频域信息
  + 离散小波变换 提取频域和时域信息
+ **降维**
  + 线性变换 PCA、SVD MDS（PCA的扩展）
  + 非线性变换 流形学习 ISOMAP、LLE、MVU、Laplacian eigenmaps、Hessian eigenmaps、Kernel PCA、Probabilistic PCA
+ **抽样**

### sparse subspace clustering 2017.11.13

**self-expressiveness** which states that each data point in a union of subspaces can be efficiently represented as a linear or affine combination of other points. Such a representation is not unique in
general because there are infinitely many ways in which a data point can be expressed as a combination of other points. The key observation is that a sparse representation of a data point ideally corresponds to a combination of a few points from its own subspace.(这种表示并不总是独一无二的，因为有无数种方法可以将数据点表示为其他点的组合。 关键的观察是，数据点的稀疏表示理想地对应于来自其自己子空间的几个点的组合。)This motivates solving a global sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of data.

**permutation matrix** 置换矩阵，是一个方形二进制矩阵，它在每行和每列中只有一个1，而在其他地方则为0。

**subspace clustering** We assume that we do not know a priori the bases of the subspaces nor do we know which data points belong to which subspace. The subspace clustering problem refers to the problem of finding the number of subspaces, their dimensions, a basis for each subspace, and the segmentation of the data from Y . **离散属性的子空间聚类** In subspace clustering, selecting correct dimensions is very important because the distance between points is easily changed according to the selected dimensions.

**subspace-sparse representation** there exists a sparse solution, $c_i$ , whose nonzero entries correspond
to data points from the same subspace as $y_I$ . We refer to such a solution as a subspace-sparse representation. More specifically, a data point $y_I$ that lies in the$d_l$-dimensional subspace $S_l$can be written as a linear combination of $d_l$ other points in general directions from $S_l$ . *As a result, ideally, a sparse representation of a data point finds points from the same subspace where the number of the nonzero elements corresponds to the dimension of the underlying subspace.*

#### FINDIT

**Dimension Voting (找相关维度的方法)**This example shows the possibility that we can estimate an original cluster’s correlated dimensions by using neighbors’ information of a point which belong to the original cluster.当维度选择过程中选择了V个最近邻居时，我们称他们为选民，因为以下过程类似于关于某个维度是否是给定点所属原始集群的相关维度的D个不同问题的投票。 要使用维度投票，我们应该找出适当的选民人数，决策的门槛和适当的大小。 确定这些值的方法写在第3节。



## Spectral clustering

**优点** 这样，谱聚类能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的**相似矩阵(拉普拉斯矩阵)**进行特征分解后得到的特征向量进行聚类。

**聚类数目的确定** Additionally, one tool which is particularly designed for spectral
clustering is the eigengap heuristic, which can be used for all three graph Laplacians. Here the goal
is to choose the number k such that all eigenvalues λ 1 , . . . , λ k are very small, but λ k+1 is relatively
large. There are several justifications for this procedure. The first one is based on perturbation theory,
where we observe that in the ideal case of k completely disconnected clusters, the eigenvalue 0 has
multiplicity k, and then there is a gap to the (k + 1)th eigenvalue λ k+1 > 0. Other explanations can
be given by spectral graph theory. Here, many geometric invariants of the graph can be expressed or
bounded with the help of the first eigenvalues of the graph Laplacian. In particular, the sizes of cuts
are closely related to the size of the first eigenvalues. For more details on this topic we refer to Bolla
(1991), Mohar (1997) and Chung (1997).

### 我的算法诊断报告

## 参考资料

1. [plukid大神的漫谈clustering系列](http://blog.pluskid.org/?p=290)
2. [知乎聚类算法总结，写的很好](https://www.zhihu.com/question/34554321)
3. [聚类论文概述](http://xueshu.baidu.com/s?wd=paperuri%3A%287d9c716d7be7778d470c5517d999ea19%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Flink.springer.com%2Fcontent%2Fpdf%2F10.1007%252Fs40745-015-0040-1.pdf&ie=utf-8&sc_us=14227662868908754060)
4. [稀疏聚类论文相关代码](http://www.vision.jhu.edu/ssc.htm)
5. [python kmeans++ 留意下](http://www.th7.cn/Program/Python/201507/498421.shtml)
6. [kmeans聚类和EM怎么联系上了](http://www.cnblogs.com/ssdut-deng/p/3399029.html)
7. [机器学习实战代码改进knn](http://www.ctolib.com/naginoasukara-machinelearninginaction.html)
8. [ng kmeans](http://blog.csdn.net/sp_programmer/article/details/42084709)
9. [谱聚类代码实现讲解MATLAB](http://blog.csdn.net/liu1194397014/article/details/52990015)
10. [谱聚类详解](https://www.cnblogs.com/Leo_wl/p/3156049.html)
11. [特别详细版的一个谱聚类](http://blog.csdn.net/yc_1993/article/details/52997074)
12. [Ng笔记及代码（简洁版）](http://memoiry.me/2017/02/22/cs229/)
13. [CLIQUE算法PPT详细阐述](https://wenku.baidu.com/view/5a2588637e21af45b307a86b.html)
14. [CLIQUE中文论文](https://wenku.baidu.com/view/396a97e65ef7ba0d4b733b07.html)
15. [CLIQUE CSDN](http://blog.csdn.net/WOJIAOSUSU/article/details/58251769?locationNum=11&fps=1)
16. [WaveCluster](https://www.qcloud.com/community/article/539270)
17. [聚类算法一览](http://blog.csdn.net/a1429331875/article/details/39058195)
18. [较直观理解谱聚类 整理时可以从这一篇开始](http://blog.csdn.net/u012771351/article/details/53213993)
19. [谱聚类原理有较好的理解](http://blog.csdn.net/pi9nc/article/details/12251247)
20. [谱聚类代码实现讲解MATLAB](http://blog.csdn.net/liu1194397014/article/details/52990015)
21. [python实现谱聚类](http://blog.csdn.net/google19890102/article/details/45697695)
22. [谱聚类实现手撸+sklearn](http://blog.csdn.net/u012500237/article/details/72864258)
23. [特别详细版的一个谱聚类](http://blog.csdn.net/yc_1993/article/details/52997074)
24. [SMO算法](http://blog.sina.com.cn/s/blog_8f6d59e40102wi89.html)
25. [Ng笔记及代码（简洁版）](http://memoiry.me/2017/02/22/cs229/)
26. [密度聚类python实现](http://blog.csdn.net/kryolith/article/details/39832573)
27. [CLIQUE中文论文](https://wenku.baidu.com/view/396a97e65ef7ba0d4b733b07.html)
28. [CLIQUE CSDN](http://blog.csdn.net/WOJIAOSUSU/article/details/58251769?locationNum=11&fps=1)


