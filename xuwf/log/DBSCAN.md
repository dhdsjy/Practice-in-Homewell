#### 相似性度量

- 距离 L1-norm 绝对值/曼哈顿距离 L2-norm 欧氏距离 Mahalanobis距离->应用于GMM
- 相似系数 主要是夹角余弦和相关系数 ，相关系数的应用比较广泛，其主要优势是不受线性变换的影响
- 核函数
- DTW

常用的距离度量

假设两个点，分别为点P和点Q，其对应的坐标分别为：

$$P=(x_1,x_2,...,x_n)\in R^n$$

$$Q=(y_1,y_2,...,y_n)\in R^n$$

**闵科夫斯基距离**：$d(P,Q)=(\sum_{i=1}^{n}(x_i-y_i)^p)^{1/p}$ (一组距离的定义)

- p=1，曼哈顿距离(city block distance)：两个点在标准坐标系上绝对轴距总和$d(P,Q)=\sum_{i=1}^{n}|x_i-y_i|$ 
- p=2，欧氏距离：两点之间最短的距离  $d(P,Q)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$      (向量形式：$d(P,Q)=\sqrt{(X-Y)(X-Y)^T}$)
- $p=\infty$，切比雪夫距离（棋盘距离）：各坐标数值差的最大值（往后从一个位置走到另一个位置的最短距离）；二维$d=max(|x_1-x_2|,|y_1-y_2|)$

![](http://img.blog.csdn.net/20160923025951361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

闵式距离缺点：1.将各个分量的量纲同等对待；2.没有考虑各个分量的分布

**标准化欧氏距离** 将各个分量都标准化到均值方差相等，假设样本集均值为m，标准差为s，标准化过程为：

$X^*=\frac{X-m}{s}$; 简单推导，可得两点间标准化欧氏距离为

$$d(P,Q)=\sqrt{\sum_{k=1}^{n} (\frac{x_i-y_i}{s_i})^2}$$

**马氏距离** 马氏距离的目的就是把方差归一化，使得特征之间的关系更加符合实际情况，表示数据的协方差距离，一种有效的计算两个未知样本集的相似度方法。与欧氏距离不同的是它考虑各种特性之间的联系，并且是尺度无关的

![](https://pic4.zhimg.com/50/v2-e2661f3587e74803540afceffc900887_hd.jpg)

左下角在二维空间中由一个分布产生的方块样本，这个分布的一条等高线如虚线的椭圆框所示，图中还有一个不属于该分布的圆圈样本。这是是一个典型的欧式距离会把分布外样本算的更近的例子，比如把绿色和蓝色样本单拎出来，就是左上角的图，蓝色小圆圈和中心的绿色方块更近了，这是因为单纯的欧式距离无法反应方块的分布。这种情况下，考虑用马氏距离。这里默认方块的分布可以由协方差矩阵很好描述（比如是个多维高斯分布），把这个协方差矩阵考虑成一个多维正态分布的协方差阵，则这个分布的密度函数的等高线，就是上面的椭圆，从椭圆中心到椭圆上各点的马氏距离，都是相等的。那对于任意两点x和y马氏距离的计算就是下面：

$$d_M(P,Q) = \sqrt{(x-y)\sum^{-1} (x-y)}$$



**夹角余弦** 用来衡量两个样本向量方向之间的差异,取值范围[-1,1],越大越相似

$$cos\theta=\frac{\sum_{i=1}^{n}x_i y_i}{\sqrt{\sum_{i=1}^{n}(x_i)^2} \sqrt{\sum_{i=1}^{n}(y_i)^2}}$$

**汉明距离** 两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需作的最小替换次数

**Jaccard 系数** 两个集合A和B的交集元素在A，B的并集中所占的比例，用来度量两个集合相似度的一种指标

$$J(A,B)=\frac{|A\cap B|}{|A\cup B|}$$

**Jaccard距离** 与jaccard系数相反的一个概念，用两个集合中不同元素占所有元素比例来衡量两个集合的区分度

$$J_\sigma (A,B)=1-J(A,B)$$

**相关系数** 衡量随机变量X与Y相关程度的一种方法，取值范围[-1,1]。相关系数绝对值越大，则表明X与Y相关度越高

$$\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\frac{E((X-EX)(Y-EY))}{\sqrt{D(X)}\sqrt{D(Y)}}$$

**相关距离**

$$D_(xy)=1-\rho_{XY} $$

**信息熵** 衡量分布的混乱程度的一种度量。分布越分散（分布越平均），信息熵就越大；分布越有序（分布越集中），信息熵就越小。

$$E(X)=\sum_{i=1}^{n}-p_i log_2 p_i$$



如果将方差的倒数看成是一个权重，该公式可看作一种加权的欧氏距离

#### 



主要优势：主要解决kmeans聚类中心初始化打问题;在聚类中心的选择过程中选择较优的聚类中心，即初始打聚类中心之间打相互距离要尽可能的远

基本思路：

+ 在数据集中选择一个样本点做为第一个初始化打聚类中心
+ 选择其余的聚类中心：
  + 计算样本中每一个样本点 i 与已经初始化的聚类中心之间的距离，并选择其中最短的距离，记为$d_{ji}$;
  + 选择一个新的数据点做为新的聚类中心，选择的原则是：$d_{ji}$较大的点（避免噪声的影响），被选取做为聚类中心的概率较大
+ 重复上述过程（2）直到k个聚类中心被选出来
+ 利用这k个初始的聚类中心来运行标准k-means

关键的一步：如何将$d_{ji}$反映到被选择的概率上，这样理解：

1. 先随机在数据集中选一个随机点做为“种子点”
2. 计算数据集中每个点和最近一个“种子点”的距离$d_{ji}$并保存在一个数组里，然后把这些距离加起来得到Sum($d$)
3. 然后，再取一个随机值，用权重的方式来取下一个“种子点”，随机值打选择Random = Sum($d$)×{0到1之间的一个数}

![](https://images2015.cnblogs.com/blog/323066/201601/323066-20160122114612468-314173187.jpg)



### DBSCAN

k-means 、k-means ++均是基于距离的聚类算法，基于距离的聚类算法的聚类结果一般是球状簇。如果聚类结果是非球状结构时，基于距离的聚类算法聚类效果并不好。而基于密度的聚类算法可以发现任意形状的聚类

基于密度聚类算法的思想：通过在数据集中寻找被低密度区域分离的高密度区域，将分离打高密度区域作为一个独立的类别

DBSCAN中两个基本参数

+ $\varepsilon$邻域: 表示在数据集D 中与样本点$x_i$的距离不大于$\varepsilon$的样本 $N_{\varepsilon}(x_i)=\{{x_j\in D |dist(x_i,x_j)\le\varepsilon}\}$
+ MinPts：表示在样本点$x_i$的$\varepsilon$ 邻域内最少样本点数目

DBSCAN算法中数据点根据以上两个参数可分为以下三类

+ 核心点 若样本$x_i$的$\varepsilon$ 邻域内至少包含来MinPts个样本，即$|N_{\varepsilon}(x_i)|\ge MinPts$,则称样本点$x_i$为核心点
+ 边界点 若样本$x_i$的$\varepsilon$ 邻域内包含的样本点数目小于MinPts，但是它是在其他核心点邻域内，则称样本点$x_i$为边界点
+ 噪音点 既不是核心点也不是边界点的点

DBSCAN 中三组概念

+ 直接密度可达 若样本点$x_j$在核心点$x_i$的$\varepsilon$ 邻域内，则称样本点$x_j$从样本点$x_i$直接密度可达
+ 密度可达 若在样本点$x_{i,1}$和样本点$x_{i,n}$之间存在序列 $x_{i,2},...,x_{i,n-1}$,且$x_{i,j+1}$从$x_{i,j}$直接密度可达，则称$x_{i,n}$从$x_{i,1}$密度可达。
+ 密度连接 对于样本点$x_i$和样本点$x_j$，若存在样本点$x_k$，使得$x_i$和$x_j$都从$x_k$密度可达，则称$x_i$和$x_j$密度相连

DBSCAN算法原理

主要思想：由核心对象出发，找到与该核心对象密度可达的所有样本形成一个聚类“簇”。

主要流程：

Input ：数据集$D = \{\vec{x_1},\vec{x_2},...,\vec{x_N}\}$;邻域参数 ($\varepsilon$,MinPts)

Output : 簇划分$C=\{C_1,C_2,...,C_K\}$

+ 初始化核心对象集合为空集：$\Omega=\Phi$
+ 根据给定的邻域参数$\varepsilon$和MinPts确定所有的核心对象：遍历所有样本点$\vec{x_i},i=1,2,...,N$, 计算$|N_{\varepsilon}(x_i)|$。如果$|N_{\varepsilon}(x_i)|\ge MinPts$，则$\Omega=\Omega \cup\{\vec{x_i}\}$
+ 迭代：以任一未访问过的核心对象为出发点，找出有其密度可达的样本生成的聚类簇，直到所有核心对象都被访问为止

[kmeans++](https://www.cnblogs.com/nocml/p/5150756.html)

[DBSCAN](https://www.cnblogs.com/pinard/p/6208966.html)